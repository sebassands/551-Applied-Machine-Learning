{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "CNN_III.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GjxnFf2Tgk4",
        "colab_type": "code",
        "outputId": "7346b670-303f-4b9b-e3b9-31e33defafb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose( #Here, we are chaining different image transformatio together using Compose\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False, num_workers=4)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "386qhY3nTglD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#New neural net with an additional layer. CONV1 --> CONV2 --> POOL1 --> CONV3 --> POOL3 --> FC1 --> FC2 --> FC3\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 96, 5) #First convolutional layer with 3 inpout channels (RGB), 96 output and 5 filter size\n",
        "        self.conv2 = nn.Conv2d(96, 32, 5) #Second convolutional layer with 96 input channels, 32 output and 5 filter size\n",
        "        self.conv3 = nn.Conv2d(32, 16, 5) #Third convolutional layer with  32 input channels, 16 output and 5 filter size\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2) # Max Pooling with 2 size and 2 strides, to be used after conv2 and again after conv3.\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120) # 4*4 are height and width of each of the 16 output channels\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10) #note that the output features for the final linear layer was choosen as 10 since we are dealing with 10 classes\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x)) # first the convolutional operations is applied for the input signal and is followed by a ReLu activation. No pooling.\n",
        "\n",
        "        x = self.pool(F.relu(self.conv2(x))) # second convolutional layer with max pooling\n",
        "  \n",
        "        x = self.pool(F.relu(self.conv3(x))) # third convolutional layer with max pooling\n",
        "\n",
        "  \n",
        "        x = x.view(-1, 16 * 4 * 4) #here we are re-shaping since we are coming from a conflate as input to a linear layer\n",
        "        #the tensor arrives to the first linear layer with a reduced size of 4x4 due to the convolution and the pooling operations.\n",
        "\n",
        "        # fully connected linear layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x) \n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net() #creating an instance of our network class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPvR0MJW2Wv8",
        "colab_type": "code",
        "outputId": "71c7632c-7d4f-45ee-daf6-c1be58a9aae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch.optim as optim \n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "!pip install livelossplot --quiet\n",
        "from livelossplot import PlotLosses\n",
        "dataloaders = {\n",
        "    \"train\": trainloader,\n",
        "    \"testing\": testloader\n",
        "}\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "def getnumcorrect (preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()\n",
        "\n",
        "\n",
        "num_epochs=30       # run over 30 epochs\n",
        "min_loss = 1        # set our minimum loss for stopping \n",
        "waiting_loss = 0    # used to calculate stopping\n",
        "n_epochs_stop = 5   # if there is no improvement on the loss for 5 straight epochs we end the process\n",
        "epochs_no_improve = 0  # counts how many epochs in a row it goes without loss improvment\n",
        "early_stop= False     # set to false to start\n",
        "correctones=[]        # initialize an array for correctones\n",
        "losses=[]             # initialize an array for losses\n",
        "liveloss = PlotLosses()\n",
        "net = net.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "        logs = {}\n",
        "        for phase in ['train', 'testing']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "            else:\n",
        "                net.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.detach() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "\n",
        "                if waiting_loss < running_loss < min_loss:\n",
        "                  epochs_no_improve += 1\n",
        "\n",
        "                if epochs_no_improve == n_epochs_stop:\n",
        "                   print('Early stopping!' )\n",
        "                   early_stop = True\n",
        "                   break\n",
        "                else:\n",
        "                   continue\n",
        "\n",
        "                waiting_loss = running_loss\n",
        "            if early_stop:\n",
        "              print(\"Stopped\")\n",
        "              break  \n",
        "\n",
        "            adjust_learning_rate(epoch)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
        "            \n",
        "            prefix = ''\n",
        "            if phase == 'testing':\n",
        "                prefix = 'val_'\n",
        "\n",
        "            logs[prefix + 'loss'] = epoch_loss.item()\n",
        "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
        "        \n",
        "        liveloss.update(logs)\n",
        "        liveloss.send()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4222, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1673, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 3228, in parseImpl\n",
            "    raise ParseException(instring, loc, self.errmsg, self)\n",
            "pip._vendor.pyparsing.ParseException: Expected W:(abcd...), found ' '  (at char 7), (line:1, col:8)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 438, in run\n",
            "    self._warn_about_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 568, in _warn_about_conflicts\n",
            "    package_set, _dep_info = check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/packaging/requirements.py\", line 93, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1929, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4430, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4037, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4749, in parseImpl\n",
            "    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4665, in parseImpl\n",
            "    loc, tokens = self_expr_parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 1669, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pyparsing.py\", line 4222, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/main.py\", line 47, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 103, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py\", line 183, in _main\n",
            "    logger.debug('Exception information:', exc_info=True)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1296, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1444, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1454, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1516, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 865, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.6/logging/handlers.py\", line 73, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 1072, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 994, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 840, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/utils/logging.py\", line 151, in format\n",
            "    formatted = super(IndentingFormatter, self).format(record)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 585, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.6/logging/__init__.py\", line 535, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 509, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 364, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.6/traceback.py\", line 286, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.6/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.6/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.6/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.6/tokenize.py\", line 454, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-86f99ade9e47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvJGYyNPTglc",
        "colab_type": "code",
        "outputId": "b7bf610e-7859-4bfa-dcc5-91741a8606ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 61 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y8sYuokTglf",
        "colab_type": "code",
        "outputId": "33b9cc93-75db-4ff1-f8df-9c72507af7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 66 %\n",
            "Accuracy of   car : 76 %\n",
            "Accuracy of  bird : 49 %\n",
            "Accuracy of   cat : 47 %\n",
            "Accuracy of  deer : 63 %\n",
            "Accuracy of   dog : 43 %\n",
            "Accuracy of  frog : 71 %\n",
            "Accuracy of horse : 61 %\n",
            "Accuracy of  ship : 77 %\n",
            "Accuracy of truck : 74 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}